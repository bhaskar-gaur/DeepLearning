{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"YOLO-HAT and Mask.ipynb","provenance":[{"file_id":"1Ict6Sw7VEOFuAlhesnY48FjMiigajxeG","timestamp":1603423738528}],"collapsed_sections":[],"mount_file_id":"1Ofuse8gJHtSDRryBLjZYsacA-vMHm9MK","authorship_tag":"ABX9TyNKdo3vYAiPAC7VoaCNUN1c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"cXzTFe5lfwzh","executionInfo":{"status":"ok","timestamp":1603627032974,"user_tz":-330,"elapsed":902,"user":{"displayName":"Vidhya Shankar V","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgibVFpmsqOpOYqW5r1kRBYXglwZsL14PgzsJPT=s64","userId":"06672572748869395626"}},"outputId":"e6203ffd-157c-4eb6-8ae7-10229426d704","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# load Google Driver\n","from google.colab import drive\n","import sys, os\n","drive.mount('/content/drive')\n","\n","my_path = '/content/drive/My Drive/Computer Vision/Extensive VisionAI (EVA5)/13_yolo_v2'\n","sys.path.append(my_path)\n","os.listdir(my_path)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['YoloV3', 'YOLO-Walle.ipynb', 'backup', 'YOLO-HAT and Mask.ipynb']"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"GjSwqgOhoE6L","executionInfo":{"status":"ok","timestamp":1603627036866,"user_tz":-330,"elapsed":871,"user":{"displayName":"Vidhya Shankar V","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgibVFpmsqOpOYqW5r1kRBYXglwZsL14PgzsJPT=s64","userId":"06672572748869395626"}},"outputId":"5909c429-3ada-447d-d31d-5f454ecd98f6","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["cd 'drive/My Drive/Computer Vision/Extensive VisionAI (EVA5)/13_yolo_v2/YoloV3'"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: 'drive/My Drive/Computer Vision/Extensive VisionAI (EVA5)/13_yolo_v2/YoloV3'\n","/content/drive/My Drive/Computer Vision/Extensive VisionAI (EVA5)/13_yolo_v2/YoloV3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M-VbtuMfxokg","executionInfo":{"status":"ok","timestamp":1603627039627,"user_tz":-330,"elapsed":843,"user":{"displayName":"Vidhya Shankar V","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgibVFpmsqOpOYqW5r1kRBYXglwZsL14PgzsJPT=s64","userId":"06672572748869395626"}}},"source":["# !git clone https://github.com/theschoolofai/YoloV3.git"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"_kGs2-3mxxCS","executionInfo":{"status":"ok","timestamp":1603627040757,"user_tz":-330,"elapsed":879,"user":{"displayName":"Vidhya Shankar V","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgibVFpmsqOpOYqW5r1kRBYXglwZsL14PgzsJPT=s64","userId":"06672572748869395626"}},"outputId":"ace1fc78-88fd-477b-fdf5-85eb8301750a","colab":{"base_uri":"https://localhost:8080/","height":100}},"source":["!ls"],"execution_count":11,"outputs":[{"output_type":"stream","text":[" annotation_tool   __pycache__\t  runs\t\t\t    train.py\n"," cfg\t\t   README.md\t  test.py\t\t   'ubdivisions=1'\n"," data\t\t   results.json   ting\t\t\t    utils\n"," detect.py\t   results.png\t  train_out\t\t    vim.exe.stackdump\n"," models.py\t   results.txt\t 'train_out (1) - images'   weights\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qTzki6PgyCJC","executionInfo":{"status":"ok","timestamp":1603627041042,"user_tz":-330,"elapsed":592,"user":{"displayName":"Vidhya Shankar V","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgibVFpmsqOpOYqW5r1kRBYXglwZsL14PgzsJPT=s64","userId":"06672572748869395626"}},"outputId":"7b85c6bb-6f6d-41a4-d947-cf4b8d6ee4bd","colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["import time\n","import glob\n","import torch\n","import os\n","\n","from IPython.display import Image, clear_output \n","print('PyTorch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["PyTorch 1.6.0+cu101 _CudaDeviceProperties(name='Tesla P4', major=6, minor=1, total_memory=7611MB, multi_processor_count=20)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4heRMJ_8ijUH","executionInfo":{"status":"ok","timestamp":1603627054391,"user_tz":-330,"elapsed":891,"user":{"displayName":"Vidhya Shankar V","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgibVFpmsqOpOYqW5r1kRBYXglwZsL14PgzsJPT=s64","userId":"06672572748869395626"}}},"source":["# !python train.py --data data/YoloV3_Dataset/train.data --batch 16 --cache --cfg cfg/yolov3-hat.cfg --epochs 100"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"YTQnQ7U1C05e","executionInfo":{"status":"ok","timestamp":1603627099467,"user_tz":-330,"elapsed":44012,"user":{"displayName":"Vidhya Shankar V","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgibVFpmsqOpOYqW5r1kRBYXglwZsL14PgzsJPT=s64","userId":"06672572748869395626"}},"outputId":"6d0c99d5-400a-4247-a69a-306fcddac4b6","colab":{"base_uri":"https://localhost:8080/","height":672}},"source":["!python train.py --data data/YoloV3_Dataset/train.data --batch 16 --cache --cfg cfg/yolov3-hat.cfg --epochs 200 --weights='weights/best.pt'"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Namespace(accumulate=4, adam=False, batch_size=16, bucket='', cache_images=True, cfg='cfg/yolov3-hat.cfg', data='data/YoloV3_Dataset/train.data', device='', epochs=200, evolve=False, img_size=[512], multi_scale=False, name='', nosave=False, notest=False, rect=False, resume=False, single_cls=False, weights='weights/best.pt')\n","Using CUDA device0 _CudaDeviceProperties(name='Tesla P4', total_memory=7611MB)\n","\n","2020-10-25 11:57:37.531034: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","Run 'tensorboard --logdir=runs' to view tensorboard at http://localhost:6006/\n","Model Summary: 225 layers, 6.25895e+07 parameters, 6.25895e+07 gradients\n","Caching labels (3032 found, 129 missing, 38 empty, 0 duplicate, for 3199 images): 100% 3199/3199 [00:02<00:00, 1090.29it/s]\n","Caching images (1.8GB): 100% 3199/3199 [00:26<00:00, 121.49it/s]\n","Caching labels (296 found, 14 missing, 7 empty, 0 duplicate, for 317 images): 100% 317/317 [00:00<00:00, 1104.49it/s]\n","Caching images (0.1GB): 100% 317/317 [00:03<00:00, 92.40it/s]\n","Image sizes 512 - 512 train, 512 test\n","Using 2 dataloader workers\n","Starting training for 200 epochs...\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n","  0% 0/200 [00:00<?, ?it/s]Traceback (most recent call last):\n","  File \"train.py\", line 430, in <module>\n","    train()  # train normally\n","  File \"train.py\", line 263, in train\n","    pred = model(imgs)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n","    result = self.forward(*input, **kwargs)\n","  File \"/content/drive/My Drive/Computer Vision/Extensive VisionAI (EVA5)/13_yolo_v2/YoloV3/models.py\", line 235, in forward\n","    return self.forward_once(x)\n","  File \"/content/drive/My Drive/Computer Vision/Extensive VisionAI (EVA5)/13_yolo_v2/YoloV3/models.py\", line 289, in forward_once\n","    x = module(x)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n","    result = self.forward(*input, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\", line 117, in forward\n","    input = module(input)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n","    result = self.forward(*input, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\", line 419, in forward\n","    return self._conv_forward(input, self.weight)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\", line 416, in _conv_forward\n","    self.padding, self.dilation, self.groups)\n","RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 7.43 GiB total capacity; 6.72 GiB already allocated; 6.94 MiB free; 6.74 GiB reserved in total by PyTorch)\n","  0% 0/200 [00:00<?, ?it/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-4Cf5zHaZv7R","executionInfo":{"status":"ok","timestamp":1603624938536,"user_tz":-330,"elapsed":1287040,"user":{"displayName":"Vidhya Shankar V","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgibVFpmsqOpOYqW5r1kRBYXglwZsL14PgzsJPT=s64","userId":"06672572748869395626"}},"outputId":"f1c40388-447f-46b9-ed80-16c0fb3ced76","colab":{"base_uri":"https://localhost:8080/","height":672}},"source":["!python train.py --data data/YoloV3_Dataset/train.data --batch 16 --cache --cfg cfg/yolov3-hat.cfg --epochs 200 --weights='weights/best.pt'"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Namespace(accumulate=4, adam=False, batch_size=16, bucket='', cache_images=True, cfg='cfg/yolov3-hat.cfg', data='data/YoloV3_Dataset/train.data', device='', epochs=200, evolve=False, img_size=[512], multi_scale=False, name='', nosave=False, notest=False, rect=False, resume=False, single_cls=False, weights='weights/best.pt')\n","Using CUDA device0 _CudaDeviceProperties(name='Tesla P4', total_memory=7611MB)\n","\n","2020-10-25 11:00:53.636584: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","Run 'tensorboard --logdir=runs' to view tensorboard at http://localhost:6006/\n","Model Summary: 225 layers, 6.25895e+07 parameters, 6.25895e+07 gradients\n","Caching labels (3032 found, 129 missing, 38 empty, 0 duplicate, for 3199 images): 100% 3199/3199 [09:04<00:00,  5.88it/s]\n","Caching images (1.8GB): 100% 3199/3199 [10:05<00:00,  5.28it/s]\n","Caching labels (296 found, 14 missing, 7 empty, 0 duplicate, for 317 images): 100% 317/317 [00:55<00:00,  5.73it/s]\n","Caching images (0.1GB): 100% 317/317 [01:01<00:00,  5.12it/s]\n","Image sizes 512 - 512 train, 512 test\n","Using 2 dataloader workers\n","Starting training for 200 epochs...\n","\n","     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n","  0% 0/200 [00:00<?, ?it/s]Traceback (most recent call last):\n","  File \"train.py\", line 430, in <module>\n","    train()  # train normally\n","  File \"train.py\", line 263, in train\n","    pred = model(imgs)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n","    result = self.forward(*input, **kwargs)\n","  File \"/content/drive/My Drive/Computer Vision/Extensive VisionAI (EVA5)/13_yolo_v2/YoloV3/models.py\", line 235, in forward\n","    return self.forward_once(x)\n","  File \"/content/drive/My Drive/Computer Vision/Extensive VisionAI (EVA5)/13_yolo_v2/YoloV3/models.py\", line 289, in forward_once\n","    x = module(x)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n","    result = self.forward(*input, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\", line 117, in forward\n","    input = module(input)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n","    result = self.forward(*input, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\", line 419, in forward\n","    return self._conv_forward(input, self.weight)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\", line 416, in _conv_forward\n","    self.padding, self.dilation, self.groups)\n","RuntimeError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 7.43 GiB total capacity; 6.72 GiB already allocated; 6.94 MiB free; 6.74 GiB reserved in total by PyTorch)\n","  0% 0/200 [00:01<?, ?it/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nlLIYRKX5bgo","executionInfo":{"status":"ok","timestamp":1603620666011,"user_tz":-330,"elapsed":9943,"user":{"displayName":"Vidhya Shankar V","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgibVFpmsqOpOYqW5r1kRBYXglwZsL14PgzsJPT=s64","userId":"06672572748869395626"}},"outputId":"8f28742b-c8a0-4d75-b845-1864be5fede7","colab":{"base_uri":"https://localhost:8080/","height":338}},"source":["!python detect.py --conf-thres 0.3 --output train_out --source 'data/vid-samples1'# --weights='weights/last.pt'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Namespace(agnostic_nms=False, augment=False, cfg='cfg/yolov3-hat.cfg', classes=None, conf_thres=0.3, device='', fourcc='mp4v', half=False, img_size=512, iou_thres=0.6, names='data/YoloV3_Dataset/classes.txt', output='train_out', save_txt=False, source='data/vid-samples1', view_img=False, weights='weights/yolov3-spp-ultralytics.pt')\n","Using CUDA device0 _CudaDeviceProperties(name='Tesla T4', total_memory=15079MB)\n","\n","Model Summary: 225 layers, 6.25895e+07 parameters, 6.25895e+07 gradients\n","Traceback (most recent call last):\n","  File \"detect.py\", line 186, in <module>\n","    detect()\n","  File \"detect.py\", line 26, in detect\n","    model.load_state_dict(torch.load(weights, map_location=device)['model'])\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 1045, in load_state_dict\n","    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n","RuntimeError: Error(s) in loading state_dict for Darknet:\n","\tsize mismatch for module_list.88.Conv2d.weight: copying a param with shape torch.Size([255, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([27, 1024, 1, 1]).\n","\tsize mismatch for module_list.88.Conv2d.bias: copying a param with shape torch.Size([255]) from checkpoint, the shape in current model is torch.Size([27]).\n","\tsize mismatch for module_list.100.Conv2d.weight: copying a param with shape torch.Size([255, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([27, 512, 1, 1]).\n","\tsize mismatch for module_list.100.Conv2d.bias: copying a param with shape torch.Size([255]) from checkpoint, the shape in current model is torch.Size([27]).\n","\tsize mismatch for module_list.112.Conv2d.weight: copying a param with shape torch.Size([255, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([27, 256, 1, 1]).\n","\tsize mismatch for module_list.112.Conv2d.bias: copying a param with shape torch.Size([255]) from checkpoint, the shape in current model is torch.Size([27]).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6RUf-zkLL51Z"},"source":[""],"execution_count":null,"outputs":[]}]}