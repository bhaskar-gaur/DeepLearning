{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EVA4 - Session 2.ipynb","provenance":[{"file_id":"1uJZvJdi5VprOQHROtJIHy0mnY2afjNlx","timestamp":1595383873255}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"4I3zNe4mYhEa","colab_type":"text"},"source":["\n","# Neural Architecture - EVA5\n","\n","\n","## *7 Steps to build a DNN using Pytorch*\n","\n","--------\n","\n","This code provides a simple steps to coding a convolutional neural network using pytorch.\n","\n","Coding with pytorch is becomes quite easy if we come to terms the building blocks of DNN and how they are put to gether. \n","\n","The list of items, if followed would cover 90% of any DNN using pytorch. The details of each program is provided as a comment or additional text. \n","\n","\n","\n","\n","1.   Import relavant libraries for pytorch (nn, functional, optim, datasets, transforms). \n","2.   Class functions for the neural architecture. \n","3.   View model summary\n","4.   Load the data (torch dataset) and use \"dataloader\" to transform (tensor, normalize, batches) the data\n","5.   Train the model (function): for each batch \n","     \n","     a. Load the data to device\n","\n","     b. forward propagate\n","\n","     c. Calculate Loss (function)\n","\n","     d. Back propagate\n","\n","     e. Update parameters\n","\n","     f. Calculate Accuracy per epoch\n","\n","6.   Test/evaluate the model (function): Test set batch\n","     \n","     a. Load the data to device\n","\n","     b. forward propagate\n","\n","     c. Predict and calculate accuracy\n","\n","\n","7.  Decide on epochs and loop through them:\n","\n","    a. Call the train function\n","\n","    b. call the test function\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"0m2JWFliFfKT","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","import torch #import torch\n","import torch.nn as nn # neural network library from torch\n","import torch.nn.functional as F # provides many functions that work like the modules we find in nn\n","import torch.optim as optim # calls optimizer that helps adjusting the model parameters(weights) example SGD, Adam etc.,\n","from torchvision import datasets, transforms # torch has inbuild datasets that can be explored. "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ax4_9um6lBia","colab_type":"text"},"source":["\"__future__\"is a pseudo-module which programmers can use to enable new language features which are not compatible with the current interpreter.\n","\n","https://stackoverflow.com/questions/7075082/what-is-future-in-python-used-for-and-how-when-to-use-it-and-how-it-works/7075121\n"]},{"cell_type":"code","metadata":{"id":"h_Cx9q2QFgM7","colab_type":"code","colab":{}},"source":["# We build a neural architecture by sublassing nn.Module. \n","#we use a subclass of nn.Module to contain our entire model. We could also use\n","#subclasses to define new building blocks for more complex networks. \n","\n","# Typically, our computation will use other modulesâ€”premade like convolutions\n","# or customized. To include these submodules, we typically define them in the constructor\n","# __init__ and assign them to self for use in the forward function.\n","# They will, at the same time, hold their parameters throughout the lifetime of our module.\n","# Note that you need to call super().__init__() before you can do that (or PyTorch will remind you)\n","\n","class Net(nn.Module): # Initialize a class Net inheriting torch's nn.Module\n","    def __init__(self):\n","        super(Net, self).__init__() # Super init is initallized so that nn.Module method is is run. Else a error is thrown\n","                                                      # Input    | conv          | output   |Receptive field\n","        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)   # 28*28*1  | (3*3*1)*32    | 28*28*32 |RF 3*3\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  # 28*28*32 | (3*3*32)*64   | 28*28*64 |RF 5*5\n","        self.pool1 = nn.MaxPool2d(2, 2)               # 28*28*64 | Max pooling   | 14*14*64 |RF 10*10\n","        self.conv3 = nn.Conv2d(64, 128, 3, padding=1) # 14*14*64 | (3*3*64)*128  | 14*14*128|RF 12*12\n","        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)# 14*14*128| (3*3*128)*256 | 14*14*256|RF 14*14\n","        self.pool2 = nn.MaxPool2d(2, 2)               # 14*14*256| Max Pooling   | 7*7*256  |RF 28*28\n","        self.conv5 = nn.Conv2d(256, 512, 3)           # 7*7*256  | (3*3*256)*512 | 5*5*512  |RF 30*30\n","        self.conv6 = nn.Conv2d(512, 1024, 3)          # 5*5*512  | (3*3*512)*1024| 3*3*1024 |RF 32*32\n","        self.conv7 = nn.Conv2d(1024, 10, 3)           # 3*3*1024 | (3*3*1024)*10 | 10*10*1  |RF 34*34\n","\n","# seperate function to keep the Functional transactions of nn module for forward propagation.\n","    \"\"\"forward\n","    Defines the computation performed at every call.\n","    Args:\n","        x: the input\n","    Returns:\n","        log_softmax(x)\n","    \"\"\" \n","    def forward(self, x): # forward function\n","        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x))))) #pool1 -> relu -> conv2 -> relu -> conv1 (x)\n","        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x))))) #pool2 -> relu -> conv4 -> relu -> conv3 \n","        x = F.relu(self.conv6(F.relu(self.conv5(x))))             # relu -> conv6 -> relu -> conv5 \n","        x = F.relu(self.conv7(x))                                 # relu -> conv7\n","        x = x.view(-1, 10)                                        # reshape and view as a vector of 10\n","        return F.log_softmax(x)                                   # return softmax applied to the vector. "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xdydjYTZFyi3","colab":{}},"source":["!pip install torchsummary\n","from torchsummary import summary\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DqTWLaM5GHgH","colab_type":"code","colab":{}},"source":["\n","\n","torch.manual_seed(1)\n","batch_size = 128\n","\n","kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=True, download=True,\n","                    transform=transforms.Compose([\n","                        transforms.ToTensor(),\n","                        transforms.Normalize((0.1307,), (0.3081,))\n","                    ])),\n","    batch_size=batch_size, shuffle=True, **kwargs)\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n","                        transforms.ToTensor(),\n","                        transforms.Normalize((0.1307,), (0.3081,))\n","                    ])),\n","    batch_size=batch_size, shuffle=True, **kwargs)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8fDefDhaFlwH","colab_type":"code","colab":{}},"source":["from tqdm import tqdm\n","def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    pbar = tqdm(train_loader)\n","    for batch_idx, (data, target) in enumerate(pbar):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMWbLWO6FuHb","colab_type":"code","colab":{}},"source":["\n","model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","for epoch in range(1, 2):\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"So5uk4EkHW6R","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}